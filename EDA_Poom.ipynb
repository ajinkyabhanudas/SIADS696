{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Extraction\n",
    "extract essential information from JSON for the analysis\n",
    "\"title\",\"views\", \"likes\",\"comments\", 'duration','tag','description', 'channel', 'category', 'published'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of channels 112\n",
      "Total number of videos 43209\n"
     ]
    }
   ],
   "source": [
    "#extracting json to df\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from datetime import datetime\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "dir = 'data'\n",
    "path = os.path.join(dir, '**/*.json')\n",
    "file_list = glob.glob(path)\n",
    "print('Total number of channels ' + str(len(file_list)))\n",
    "# print(file_list)\n",
    "dfs = list()\n",
    "\n",
    "for file in file_list:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    channel_id, stats = data.popitem()\n",
    "    pchannel_stats = stats[\"channel_statistics\"]\n",
    "    video_stats = stats[\"video_data\"]\n",
    "    # sorted_vids = sorted(video_stats.items(), key=lambda item: int(item[1][\"viewCount\"]),reverse=True)\n",
    "    vids = video_stats.items()\n",
    "    stats = []\n",
    "    for vid in vids:\n",
    "        video_id = vid[0]\n",
    "        title = vid[1][\"title\"]\n",
    "        try:\n",
    "            views = vid[1][\"viewCount\"]\n",
    "            likes = vid[1][\"likeCount\"]\n",
    "            duration = vid[1]['duration']\n",
    "            tags = vid[1]['tags']\n",
    "            description = vid[1]['description']\n",
    "            comments = vid[1][\"commentCount\"]\n",
    "            channel = vid[1]['channelTitle']\n",
    "            published = vid[1]['publishedAt'].split('T')[0]\n",
    "        except:\n",
    "            pass\n",
    "        cat = os.path.dirname(file).split('\\\\')[1]\n",
    "        stats.append([title,views, published, likes, comments, duration, tags, description, channel, cat])\n",
    "    vid_df = pd.DataFrame(stats, columns=[\"title\",\"views\", 'published',\"likes\",\"comments\", 'duration','tag','description', 'channel', 'category'])\n",
    "    dfs.append(vid_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "print('Total number of videos ' + str(df.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43209 entries, 0 to 43208\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   title        43209 non-null  object\n",
      " 1   views        43209 non-null  object\n",
      " 2   published    43209 non-null  object\n",
      " 3   likes        43209 non-null  object\n",
      " 4   comments     43209 non-null  object\n",
      " 5   duration     43209 non-null  object\n",
      " 6   tag          43209 non-null  object\n",
      " 7   description  43209 non-null  object\n",
      " 8   channel      43209 non-null  object\n",
      " 9   category     43209 non-null  object\n",
      "dtypes: object(10)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>views</th>\n",
       "      <th>published</th>\n",
       "      <th>likes</th>\n",
       "      <th>comments</th>\n",
       "      <th>duration</th>\n",
       "      <th>tag</th>\n",
       "      <th>description</th>\n",
       "      <th>channel</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39177</th>\n",
       "      <td>Lake Como, Italy: Bellagio and Varenna</td>\n",
       "      <td>525512</td>\n",
       "      <td>2012-05-10</td>\n",
       "      <td>2135</td>\n",
       "      <td>77</td>\n",
       "      <td>PT3M1S</td>\n",
       "      <td>[Rick Steves, Milano, rick steves lake como, l...</td>\n",
       "      <td>More info about travel to the Italian Lakes: h...</td>\n",
       "      <td>Rick Steves' Europe</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26787</th>\n",
       "      <td>By the Numbers: Thousands of students ‘missing...</td>\n",
       "      <td>6538</td>\n",
       "      <td>2021-06-12</td>\n",
       "      <td>100</td>\n",
       "      <td>31</td>\n",
       "      <td>PT1M35S</td>\n",
       "      <td>[abcnl, by, from, many, missing, numbers, of, ...</td>\n",
       "      <td>A look at the thousands of students who fell o...</td>\n",
       "      <td>ABC News</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4728</th>\n",
       "      <td>Cool Guys Don't Look At Explosions</td>\n",
       "      <td>50205179</td>\n",
       "      <td>2009-06-05</td>\n",
       "      <td>363087</td>\n",
       "      <td>39897</td>\n",
       "      <td>PT2M31S</td>\n",
       "      <td>[Andy, Samberg, 2009, MTV, Movie, Awards, SNL,...</td>\n",
       "      <td>Here's the song we made for the 2009 MTV Movie...</td>\n",
       "      <td>thelonelyisland</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28058</th>\n",
       "      <td>Sean Penn helps Ukrainian fighter pilots lobby...</td>\n",
       "      <td>68014</td>\n",
       "      <td>2022-06-25</td>\n",
       "      <td>2166</td>\n",
       "      <td>1620</td>\n",
       "      <td>PT3M24S</td>\n",
       "      <td>[latest News, Happening Now, CNN, Moonfish, Ju...</td>\n",
       "      <td>CNN's Jim Acosta speaks to actor Sean Penn and...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15316</th>\n",
       "      <td>What's the Difference Between Baking Powder an...</td>\n",
       "      <td>1868105</td>\n",
       "      <td>2016-11-14</td>\n",
       "      <td>34250</td>\n",
       "      <td>954</td>\n",
       "      <td>PT5M8S</td>\n",
       "      <td>[SciShow, science, Hank, Green, education, lea...</td>\n",
       "      <td>Powder vs. Soda: an important distinction!\\n\\n...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>Educational</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title     views  published  \\\n",
       "39177             Lake Como, Italy: Bellagio and Varenna    525512 2012-05-10   \n",
       "26787  By the Numbers: Thousands of students ‘missing...      6538 2021-06-12   \n",
       "4728                  Cool Guys Don't Look At Explosions  50205179 2009-06-05   \n",
       "28058  Sean Penn helps Ukrainian fighter pilots lobby...     68014 2022-06-25   \n",
       "15316  What's the Difference Between Baking Powder an...   1868105 2016-11-14   \n",
       "\n",
       "        likes comments duration  \\\n",
       "39177    2135       77   PT3M1S   \n",
       "26787     100       31  PT1M35S   \n",
       "4728   363087    39897  PT2M31S   \n",
       "28058    2166     1620  PT3M24S   \n",
       "15316   34250      954   PT5M8S   \n",
       "\n",
       "                                                     tag  \\\n",
       "39177  [Rick Steves, Milano, rick steves lake como, l...   \n",
       "26787  [abcnl, by, from, many, missing, numbers, of, ...   \n",
       "4728   [Andy, Samberg, 2009, MTV, Movie, Awards, SNL,...   \n",
       "28058  [latest News, Happening Now, CNN, Moonfish, Ju...   \n",
       "15316  [SciShow, science, Hank, Green, education, lea...   \n",
       "\n",
       "                                             description              channel  \\\n",
       "39177  More info about travel to the Italian Lakes: h...  Rick Steves' Europe   \n",
       "26787  A look at the thousands of students who fell o...             ABC News   \n",
       "4728   Here's the song we made for the 2009 MTV Movie...      thelonelyisland   \n",
       "28058  CNN's Jim Acosta speaks to actor Sean Penn and...                  CNN   \n",
       "15316  Powder vs. Soda: an important distinction!\\n\\n...              SciShow   \n",
       "\n",
       "          category  \n",
       "39177       Travel  \n",
       "26787         News  \n",
       "4728        Comedy  \n",
       "28058         News  \n",
       "15316  Educational  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert published date into datetime\n",
    "df.published = pd.to_datetime(df.published, format='%Y-%m-%d')\n",
    "\n",
    "# convert duration into minute with regex\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert text into Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# import nltk; nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use gensim pretrained model called 'glove-wiki-gigaword-50' with the model trained on wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pretrained gensim model to convert text tokens into vector\n",
    "model_wv = api.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert token to word vector with word2vec finding average vector for the document\n",
    "# tokenize function with df series as input to give a list of list result of each title\n",
    "def df_token(df_text_series):\n",
    "    # set of english stopwords\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    token_item = []\n",
    "    for t in df_text_series:\n",
    "        if type(t) == list:\n",
    "            t = ' '.join(t)\n",
    "        else:\n",
    "            pass\n",
    "        tokens = [word for word in word_tokenize(t.lower()) if word not in stopset]\n",
    "        token_item.append(tokens)\n",
    "    return token_item\n",
    "\n",
    "def gen_word_vec(df_text_series, wordvec):\n",
    "    word_vector = []\n",
    "    for token in df_token(df_text_series):\n",
    "        token_considered = [t for t in token if t.isalpha]\n",
    "        token_vocab = [i for i in token_considered if i in wordvec.key_to_index]\n",
    "        if len(token_vocab) > 0:\n",
    "            word_vector.append(np.mean(wordvec[token_vocab], axis=0))\n",
    "        else:\n",
    "            word_vector.append(np.zeros(wordvec.vector_size))\n",
    "    word_vector = np.array(word_vector)\n",
    "    return word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_word_vec = gen_word_vec(df.title, model_wv)\n",
    "tag_word_vec = gen_word_vec(df.tag, model_wv)\n",
    "desc_word_vec = gen_word_vec(df.description, model_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add this vector to the original df and see the similarity within the channel\n",
    "df['title_vector'] = title_word_vec.tolist()\n",
    "df['tag_vector'] = tag_word_vec.tolist()\n",
    "df['desc_vector'] = desc_word_vec.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>views</th>\n",
       "      <th>published</th>\n",
       "      <th>likes</th>\n",
       "      <th>comments</th>\n",
       "      <th>duration</th>\n",
       "      <th>tag</th>\n",
       "      <th>description</th>\n",
       "      <th>channel</th>\n",
       "      <th>category</th>\n",
       "      <th>title_vector</th>\n",
       "      <th>tag_vector</th>\n",
       "      <th>desc_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15344</th>\n",
       "      <td>The myth of Zeus' test - Iseult Gillespie</td>\n",
       "      <td>438199</td>\n",
       "      <td>2022-09-13</td>\n",
       "      <td>19485</td>\n",
       "      <td>696</td>\n",
       "      <td>PT6M9S</td>\n",
       "      <td>[zeus, hermes, baucis, philemon, baucis and ph...</td>\n",
       "      <td>Dig into the myth of Baucis and Philemon, a co...</td>\n",
       "      <td>TED-Ed</td>\n",
       "      <td>Educational</td>\n",
       "      <td>[0.1946544349193573, 0.38024282455444336, -0.1...</td>\n",
       "      <td>[0.11282431334257126, 0.6269444227218628, -0.3...</td>\n",
       "      <td>[0.2323508858680725, 0.5799580216407776, 0.295...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12240</th>\n",
       "      <td>Introduction to improper integrals | AP Calcul...</td>\n",
       "      <td>526746</td>\n",
       "      <td>2014-06-06</td>\n",
       "      <td>1151</td>\n",
       "      <td>91</td>\n",
       "      <td>PT3M52S</td>\n",
       "      <td>[dotsub]</td>\n",
       "      <td>Learn more about ATP: how it stores energy, an...</td>\n",
       "      <td>Khan Academy</td>\n",
       "      <td>Educational</td>\n",
       "      <td>[-0.3846859931945801, 0.6127420663833618, 0.14...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.03771638125181198, 0.39429447054862976, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33397</th>\n",
       "      <td>How sea sponges are harvested #shorts</td>\n",
       "      <td>16776</td>\n",
       "      <td>2022-02-09</td>\n",
       "      <td>537</td>\n",
       "      <td>51</td>\n",
       "      <td>PT1M1S</td>\n",
       "      <td>[science, tech, science insider]</td>\n",
       "      <td>#sponges #seasponge #scienceinsider\\n\\nHow sea...</td>\n",
       "      <td>Science Insider</td>\n",
       "      <td>Science</td>\n",
       "      <td>[0.04434001445770264, 0.05265200138092041, -0....</td>\n",
       "      <td>[-0.6982625722885132, 0.4484702944755554, 0.15...</td>\n",
       "      <td>[-0.08555710315704346, 0.2091914266347885, 0.3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title   views  published  \\\n",
       "15344          The myth of Zeus' test - Iseult Gillespie  438199 2022-09-13   \n",
       "12240  Introduction to improper integrals | AP Calcul...  526746 2014-06-06   \n",
       "33397              How sea sponges are harvested #shorts   16776 2022-02-09   \n",
       "\n",
       "       likes comments duration  \\\n",
       "15344  19485      696   PT6M9S   \n",
       "12240   1151       91  PT3M52S   \n",
       "33397    537       51   PT1M1S   \n",
       "\n",
       "                                                     tag  \\\n",
       "15344  [zeus, hermes, baucis, philemon, baucis and ph...   \n",
       "12240                                           [dotsub]   \n",
       "33397                   [science, tech, science insider]   \n",
       "\n",
       "                                             description          channel  \\\n",
       "15344  Dig into the myth of Baucis and Philemon, a co...           TED-Ed   \n",
       "12240  Learn more about ATP: how it stores energy, an...     Khan Academy   \n",
       "33397  #sponges #seasponge #scienceinsider\\n\\nHow sea...  Science Insider   \n",
       "\n",
       "          category                                       title_vector  \\\n",
       "15344  Educational  [0.1946544349193573, 0.38024282455444336, -0.1...   \n",
       "12240  Educational  [-0.3846859931945801, 0.6127420663833618, 0.14...   \n",
       "33397      Science  [0.04434001445770264, 0.05265200138092041, -0....   \n",
       "\n",
       "                                              tag_vector  \\\n",
       "15344  [0.11282431334257126, 0.6269444227218628, -0.3...   \n",
       "12240  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "33397  [-0.6982625722885132, 0.4484702944755554, 0.15...   \n",
       "\n",
       "                                             desc_vector  \n",
       "15344  [0.2323508858680725, 0.5799580216407776, 0.295...  \n",
       "12240  [0.03771638125181198, 0.39429447054862976, -0....  \n",
       "33397  [-0.08555710315704346, 0.2091914266347885, 0.3...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dataenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "467f60c46b88ea196e8e4e51716c4f14b05141d893e1660e0038b81da854c476"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
