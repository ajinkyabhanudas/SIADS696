{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction\n",
    "extract essential information from JSON for the analysis\n",
    "\"title\",\"views\", \"likes\",\"comments\", 'duration','tag','description', 'channel', 'category', 'published'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\poomk\\OneDrive\\Desktop\\Youtube_Analysis\\SIADS696\\Unsupervised_ML.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/poomk/OneDrive/Desktop/Youtube_Analysis/SIADS696/Unsupervised_ML.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/poomk/OneDrive/Desktop/Youtube_Analysis/SIADS696/Unsupervised_ML.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/poomk/OneDrive/Desktop/Youtube_Analysis/SIADS696/Unsupervised_ML.ipynb#W1sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdownloader\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mapi\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/poomk/OneDrive/Desktop/Youtube_Analysis/SIADS696/Unsupervised_ML.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/poomk/OneDrive/Desktop/Youtube_Analysis/SIADS696/Unsupervised_ML.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\poomk\\miniconda3\\envs\\dataenv\\lib\\site-packages\\gensim\\__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m4.2.0\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[39m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m'\u001b[39m\u001b[39mgensim\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m logger\u001b[39m.\u001b[39mhandlers:  \u001b[39m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\poomk\\miniconda3\\envs\\dataenv\\lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mindexedcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m IndexedCorpus  \u001b[39m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmmcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m MmCorpus  \u001b[39m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbleicorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m BleiCorpus  \u001b[39m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\poomk\\miniconda3\\envs\\dataenv\\lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mIndexedCorpus\u001b[39;00m(interfaces\u001b[39m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32mc:\\Users\\poomk\\miniconda3\\envs\\dataenv\\lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[39mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCorpusABC\u001b[39;00m(utils\u001b[39m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32mc:\\Users\\poomk\\miniconda3\\envs\\dataenv\\lib\\site-packages\\gensim\\matutils.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m \u001b[39mimport\u001b[39;00m entropy\n\u001b[0;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlapack\u001b[39;00m \u001b[39mimport\u001b[39;00m get_lapack_funcs\n",
      "File \u001b[1;32mc:\\Users\\poomk\\miniconda3\\envs\\dataenv\\lib\\site-packages\\scipy\\stats\\__init__.py:441\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    438\u001b[0m \n\u001b[0;32m    439\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 441\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    442\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    443\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmorestats\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\poomk\\miniconda3\\envs\\dataenv\\lib\\site-packages\\scipy\\stats\\stats.py:43\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspecial\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m linalg\n\u001b[1;32m---> 43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m distributions\n\u001b[0;32m     44\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m mstats_basic\n\u001b[0;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_mstats_common\u001b[39;00m \u001b[39mimport\u001b[39;00m (_find_repeats, linregress, theilslopes,\n\u001b[0;32m     46\u001b[0m                                    siegelslopes)\n",
      "File \u001b[1;32mc:\\Users\\poomk\\miniconda3\\envs\\dataenv\\lib\\site-packages\\scipy\\stats\\distributions.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Author:  Travis Oliphant  2002-2011 with contributions from\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m#          SciPy Developers 2004-2011\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39m#       instead of `git blame -Lxxx,+x`.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_distn_infrastructure\u001b[39;00m \u001b[39mimport\u001b[39;00m (rv_discrete, rv_continuous, rv_frozen)\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _continuous_distns\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _discrete_distns\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_continuous_distns\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\poomk\\miniconda3\\envs\\dataenv\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py:5953\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   5949\u001b[0m         outputs \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m a \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mnan \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m5\u001b[39m)]\n\u001b[0;32m   5950\u001b[0m         \u001b[39mreturn\u001b[39;00m outputs[:]\n\u001b[1;32m-> 5953\u001b[0m kappa3 \u001b[39m=\u001b[39m kappa3_gen(a\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m, name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mkappa3\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m   5956\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mmoyal_gen\u001b[39;00m(rv_continuous):\n\u001b[0;32m   5957\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"A Moyal continuous random variable.\u001b[39;00m\n\u001b[0;32m   5958\u001b[0m \n\u001b[0;32m   5959\u001b[0m \u001b[39m    %(before_notes)s\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5997\u001b[0m \n\u001b[0;32m   5998\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\poomk\\miniconda3\\envs\\dataenv\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1721\u001b[0m, in \u001b[0;36mrv_continuous.__init__\u001b[1;34m(self, momtype, a, b, xtol, badvalue, name, longname, shapes, extradoc, seed)\u001b[0m\n\u001b[0;32m   1716\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextradoc \u001b[39m=\u001b[39m extradoc\n\u001b[0;32m   1718\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_argparser(meths_to_inspect\u001b[39m=\u001b[39m[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pdf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cdf],\n\u001b[0;32m   1719\u001b[0m                           locscale_in\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloc=0, scale=1\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1720\u001b[0m                           locscale_out\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloc, scale\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 1721\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attach_methods()\n\u001b[0;32m   1723\u001b[0m \u001b[39mif\u001b[39;00m longname \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1724\u001b[0m     \u001b[39mif\u001b[39;00m name[\u001b[39m0\u001b[39m] \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39maeiouAEIOU\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\poomk\\miniconda3\\envs\\dataenv\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1768\u001b[0m, in \u001b[0;36mrv_continuous._attach_methods\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1766\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneric_moment \u001b[39m=\u001b[39m vectorize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mom0_sc, otypes\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39md\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1767\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1768\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneric_moment \u001b[39m=\u001b[39m vectorize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mom1_sc, otypes\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39md\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m   1769\u001b[0m \u001b[39m# Because of the *args argument of _mom0_sc, vectorize cannot count the\u001b[39;00m\n\u001b[0;32m   1770\u001b[0m \u001b[39m# number of arguments correctly.\u001b[39;00m\n\u001b[0;32m   1771\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneric_moment\u001b[39m.\u001b[39mnin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumargs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\poomk\\miniconda3\\envs\\dataenv\\lib\\site-packages\\numpy\\lib\\function_base.py:2271\u001b[0m, in \u001b[0;36mvectorize.__init__\u001b[1;34m(self, pyfunc, otypes, doc, excluded, cache, signature)\u001b[0m\n\u001b[0;32m   2268\u001b[0m     excluded \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m   2269\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexcluded \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(excluded)\n\u001b[1;32m-> 2271\u001b[0m \u001b[39mif\u001b[39;00m signature \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m:\n\u001b[0;32m   2272\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_and_out_core_dims \u001b[39m=\u001b[39m _parse_gufunc_signature(signature)\n\u001b[0;32m   2273\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#extracting json to df\n",
    "import os, json, re\n",
    "import enum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from itertools import groupby\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize ,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# import nltk; nltk.download('popular')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "warnings.simplefilter(action='ignore')\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = 'data'\n",
    "feature_list = ['publishedAt', 'title', 'channelId', 'description',\n",
    "                'channelTitle', 'tags', 'categoryId', 'viewCount', \n",
    "                'likeCount', 'favoriteCount', 'commentCount', 'duration',\n",
    "                'definition', 'contentRating', 'topicCategories', 'topicLabel']\n",
    "\n",
    "ov_dict_list = []\n",
    "ov_temp_dict = {}\n",
    "\n",
    "dict_list = []\n",
    "temp_dict = {}\n",
    "\n",
    "for path in glob.glob(f'./{rootdir}/*/*'):\n",
    "  try:\n",
    "    with open(path, \"r\") as read_file:\n",
    "      data = json.load(read_file)\n",
    "    for channel_id in data.keys():\n",
    "      ov_temp_dict = data[channel_id][\"channel_statistics\"]\n",
    "      channel_name = list(data[channel_id][\"video_data\"].keys())[0]\n",
    "      ov_temp_dict[\"channelName\"] = data[channel_id][\"video_data\"][channel_name][\"channelTitle\"]\n",
    "      ov_dict_list.append(ov_temp_dict)\n",
    "      \n",
    "      for video_info in data[channel_id][\"video_data\"].values():\n",
    "        temp_dict = video_info\n",
    "        temp_dict[\"topicLabel\"] = path.split('\\\\')[1]\n",
    "        dict_list.append(temp_dict)       \n",
    "  except:\n",
    "    pass\n",
    "\n",
    "overall_stats_raw_df = pd.DataFrame(ov_dict_list)\n",
    "overall_stats_raw_df.drop(\"hiddenSubscriberCount\", axis=1, inplace=True)\n",
    "\n",
    "raw_df = pd.DataFrame(dict_list)\n",
    "raw_df = raw_df[feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_df.channelTitle.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration_split(duration):\n",
    "  try:\n",
    "    for _, v in groupby(duration, str.isalpha):\n",
    "      yield ''.join(v)\n",
    "  except:\n",
    "    yield np.nan\n",
    "    \n",
    "    \n",
    "def duration_2_secs(duration, duration_split=duration_split):\n",
    "  temp = 0\n",
    "  # value = (duration_split(duration))\n",
    "  for i, val in enumerate(duration_split(duration)):\n",
    "    \n",
    "    if len(str(duration)) <= 8:\n",
    "      if i==1 and not val.isalpha():\n",
    "        temp += float(val)*60\n",
    "      \n",
    "      if i==3 and not val.isalpha():\n",
    "        temp += float(val)\n",
    "    else:\n",
    "      if i==1 and not val.isalpha():\n",
    "        temp += float(val)*60*60\n",
    "      \n",
    "      if i==3 and not val.isalpha():\n",
    "        temp += float(val)*60\n",
    "      \n",
    "      if i==5 and not val.isalpha():\n",
    "        temp += float(val)\n",
    "\n",
    "  return(temp)\n",
    "\n",
    "\n",
    "# duration_2_secs(\"HR1PT51M12S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_extract(links_list):\n",
    "  \n",
    "  topics_list = []  \n",
    "  try:\n",
    "    for link in links_list:\n",
    "      topics_list.append(link.split(\"/\")[-1].lower())\n",
    "    \n",
    "    return(topics_list)\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prep(val):\n",
    "  '''the goal is to replace the hypertexts in the \n",
    "  in any field to redundant names as vectorizing\n",
    "  them could be misleading and also leak data'''\n",
    "  val = str(val).lower()\n",
    "  process_desc = re.sub(r'http[s]*:.*\\w', 'url', val)\n",
    "  process_desc = re.sub('[^a-zA-Z]', ' ', process_desc)\n",
    "  process_desc = re.sub(r'\\s+', ' ', process_desc)\n",
    "\n",
    "  process_desc = nltk.sent_tokenize(process_desc)\n",
    "  if not process_desc:\n",
    "    process_desc = [nltk.word_tokenize(word) for word in process_desc]\n",
    "  else:\n",
    "    process_desc = [nltk.word_tokenize(word) for word in process_desc][0]\n",
    "    \n",
    "  \n",
    "  word_list = [word for word in process_desc if word not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "  return word_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, featureset, primary=True):\n",
    "\n",
    "  if primary:\n",
    "    df[\"publishedAt\"] = pd.to_datetime(df.publishedAt, format='%Y-%m-%d')\n",
    "    df['publishedDayDelta'] = (datetime.now(timezone.utc) - df['publishedAt']).apply(lambda x: x.days)\n",
    "    df[\"categoryId\"] = df.categoryId.astype(float)\n",
    "    df[\"viewCount\"] = df.viewCount.astype(float)\n",
    "    df[\"likeCount\"] = df.likeCount.astype(float)\n",
    "    df[\"favoriteCount\"] = df.favoriteCount.astype(float)\n",
    "    df[\"commentCount\"] = df.commentCount.astype(float)\n",
    "    df['duration_secs'] = df.duration.apply(lambda x: duration_2_secs(x))\n",
    "    df['topicCategories'] = df.topicCategories.apply(lambda x: topic_extract(x))\n",
    "    df['channelTitle'] = df.channelTitle.str.lower()\n",
    "    df['topicLabel'] = df.topicLabel.str.lower()\n",
    "    df['log_duration_secs'] = np.log(df.duration_secs+1)\n",
    "  \n",
    "  else:\n",
    "    df['ov_viewCount'] = df.viewCount.astype(float)\n",
    "    df['ov_subscriberCount'] = df.subscriberCount.astype(float)\n",
    "    df['ov_videoCount'] = df.videoCount.astype(float)\n",
    "    df['channelName'] = df.channelName.str.lower()\n",
    "\n",
    "  return df[featureset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "  'title', 'description',\n",
    "  'channelTitle', 'categoryId', 'viewCount', \n",
    "  'log_duration_secs', 'topicCategories', 'publishedDayDelta', 'topicLabel'\n",
    " ]\n",
    "\n",
    "prep_df = create_dataset(raw_df, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_na = prep_df.isna().sum()/len(prep_df)*100\n",
    "pct_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_dropna = len(prep_df.dropna())/len(prep_df)*100\n",
    "pct_dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_df = prep_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(prep_df[['categoryId', 'viewCount', 'log_duration_secs', 'publishedDayDelta']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(prep_df.corr(), cmap='rocket_r', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Mining\n",
    "convert text into Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use gensim pretrained model called 'glove-wiki-gigaword-50' with the model trained on wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pretrained gensim model to convert text tokens into vector\n",
    "model_wv = api.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert token to word vector with word2vec finding average vector for the document\n",
    "# tokenize function with df series as input to give a list of list result of each title\n",
    "def df_token(df_text_series):\n",
    "    # set of english stopwords\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    token_item = []\n",
    "    for t in df_text_series:\n",
    "        if type(t) == list:\n",
    "            t = str(' '.join(t))\n",
    "        else:\n",
    "            pass\n",
    "        try:\n",
    "            tokens = [word for word in word_tokenize(t.lower()) if word not in stopset]\n",
    "        except:\n",
    "            tokens == None\n",
    "        token_item.append(tokens)\n",
    "    return token_item\n",
    "\n",
    "def gen_word_vec(df_text_series, wordvec):\n",
    "    word_vector = []\n",
    "    for token in df_token(df_text_series):\n",
    "        token_considered = [t for t in token if t.isalpha]\n",
    "        token_vocab = [i for i in token_considered if i in wordvec.key_to_index]\n",
    "        if len(token_vocab) > 0:\n",
    "            word_vector.append(np.mean(wordvec[token_vocab], axis=0))\n",
    "        else:\n",
    "            word_vector.append(np.zeros(wordvec.vector_size))\n",
    "    word_vector = np.array(word_vector)\n",
    "    return word_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_word_vec = gen_word_vec(prep_df.title, model_wv)\n",
    "desc_word_vec = gen_word_vec(prep_df.description, model_wv)\n",
    "topic_word_vec = gen_word_vec(prep_df.topicCategories, model_wv)\n",
    "label_word_vec = gen_word_vec(prep_df.topicLabel, model_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dimension of the average word vector for each title\n",
    "title_word_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracting cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "A = title_word_vec\n",
    "vectors = da.from_array(A, 10000)\n",
    "cosine = cosine_similarity(vectors)\n",
    "avg_cosine_dask = []\n",
    "for i in range(A.shape[0]):\n",
    "    avg_cosine_dask.append(np.mean(cosine[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce dimensionality using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate PCA with 1 components\n",
    "pca = PCA(n_components=1)\n",
    "title_pc = pca.fit_transform(title_word_vec)\n",
    "desc_pc = pca.fit_transform(desc_word_vec)\n",
    "topic_pc = pca.fit_transform(topic_word_vec)\n",
    "label_pc = pca.fit_transform(label_word_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy prep_df to be worked with\n",
    "vec_df = prep_df.copy()\n",
    "#replace df with each vector PCs\n",
    "vec_df['titleVec'] = title_pc\n",
    "vec_df['label'] = label_pc\n",
    "vec_df['description'] = desc_pc\n",
    "vec_df['topicCategories'] = topic_pc\n",
    "vec_df['avgTitleCosine'] =avg_cosine_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_vec = vec_df.drop(columns=['channelTitle', 'title', 'topicLabel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_vec.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_view_vec = vec_df.copy()\n",
    "norm_view_vec['averageView'] = norm_view_vec.viewCount/norm_view_vec.publishedDayDelta\n",
    "norm_view_vec['normalizedScore'] = norm_view_vec.groupby('topicLabel').averageView.apply(lambda x: x/x.median())\n",
    "norm_view_vec.drop(['channelTitle', 'viewCount', 'averageView' , 'title', 'topicLabel'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_view_vec.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the df to be fitted with PCA\n",
    "scaler = StandardScaler()\n",
    "scale_view_vec = scaler.fit_transform(view_vec)\n",
    "scale_norm_view_vec = scaler.fit_transform(norm_view_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_norm_view_df = pd.DataFrame(scale_norm_view_vec, columns=norm_view_vec.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(scale_norm_view_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "sns.heatmap(ax=ax, data=scale_norm_view_df.corr(), cmap='coolwarm_r', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create biplot source from https://sukhbinder.wordpress.com/2015/08/05/biplot-with-python/ + unsupervise learning class\n",
    "def biplot(score, coeff, maxdim, pcax=1, pcay=2, labels=None):\n",
    "    '''\n",
    "    score: pca fit_transform\n",
    "    coeff: components.transpose\n",
    "    '''\n",
    "    pca1=pcax-1\n",
    "    pca2=pcay-1\n",
    "    xs = score[:,pca1]\n",
    "    ys = score[:,pca2]\n",
    "    n = min(coeff.shape[0], maxdim)\n",
    "    scalex = 2.0/(xs.max()- xs.min())\n",
    "    scaley = 2.0/(ys.max()- ys.min())\n",
    "    text_scale_factor = 1.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(xs*scalex, ys*scaley, s=1)\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coeff[i,pca1], coeff[i,pca2],color='r',alpha=0.5) \n",
    "        if labels is None:\n",
    "            plt.text(coeff[i,pca1]* text_scale_factor, coeff[i,pca2] * text_scale_factor, \"Var\"+str(i+1), color='k', ha='center', va='center')\n",
    "        else:\n",
    "            plt.text(coeff[i,pca1]* text_scale_factor, coeff[i,pca2], labels[i], color='m', ha='center', va='center')\n",
    "    plt.xlim(-1,1)\n",
    "    plt.ylim(-1,1)\n",
    "    plt.xlabel(\"PC{}\".format(pcax))\n",
    "    plt.ylabel(\"PC{}\".format(pcay))\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biplot with viewCount variable\n",
    "score_pca = PCA(n_components=2)\n",
    "score = score_pca.fit_transform(scale_view_vec)\n",
    "labels = view_vec.columns\n",
    "coeff = np.transpose(score_pca.components_)\n",
    "biplot(score,coeff, maxdim=12, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#biplot normalizedScore variable\n",
    "score_pca = PCA(n_components=8).fit(scale_norm_view_vec)\n",
    "score = score_pca.transform(scale_norm_view_vec)\n",
    "labels = scale_norm_view_df.columns\n",
    "coeff = np.transpose(score_pca.components_)\n",
    "biplot(score,coeff, maxdim=12, labels=labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scree plot\n",
    "PC_values = np.arange(score_pca.n_components_) + 1\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(PC_values, score_pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the clustering of word embadding vector using K-Means\n",
    "# first let's find optimal number of cluster using elbow methods\n",
    "\n",
    "# workingin title word embedding\n",
    "sse=[] # sum of square error\n",
    "list_k = list(range(1, 50))\n",
    "\n",
    "X = scale_norm_view_vec\n",
    "for k in list_k:\n",
    "    km = MiniBatchKMeans(n_clusters=k, init='k-means++', max_iter=300, random_state=0, batch_size=2048)\n",
    "    km.fit(X)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore cluster with yellowbrick KElbowVisualizer\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "X = scale_norm_view_vec\n",
    "model = MiniBatchKMeans(n_clusters=k, init='k-means++', max_iter=300, random_state=0, batch_size=2048)\n",
    "visualizer = KElbowVisualizer(model, k=(3,13))\n",
    "visualizer.fit(X)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result from the elbow method shows that the optimum number of KNN is 9, so let's explore further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silhouette score plot with yellowbrick\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "model_k = MiniBatchKMeans(n_clusters=9, init='k-means++', max_iter=300, random_state=0, batch_size=2048)\n",
    "visualizer_silhouette = SilhouetteVisualizer(model_k, colors='yellowbrick')\n",
    "visualizer_silhouette.fit(X)\n",
    "visualizer_silhouette.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use silhouette plot based on sklearn documentation page https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "#set the dataset to be working with\n",
    "X = scale_norm_view_vec\n",
    "\n",
    "for i, k in enumerate([5, 6, 7, 8, 9, 10, 11]):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "    \n",
    "    # Run the Kmeans algorithm\n",
    "    kmeans = MiniBatchKMeans(n_clusters=k, init='k-means++', max_iter=300, random_state=0, batch_size=2048)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    # centroids = kmeans.cluster_centers_\n",
    "\n",
    "    # Get silhouette samples\n",
    "    silhouette_vals = silhouette_samples(X, labels)\n",
    "\n",
    "    # Silhouette plot\n",
    "    y_ticks = []\n",
    "    y_lower, y_upper = 0, 0\n",
    "    for i, cluster in enumerate(np.unique(labels)):\n",
    "        cluster_silhouette_vals = silhouette_vals[labels == cluster]\n",
    "        cluster_silhouette_vals.sort()\n",
    "        y_upper += len(cluster_silhouette_vals)\n",
    "        ax1.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none', height=1)\n",
    "        ax1.text(-0.03, (y_lower + y_upper) / 2, str(i + 1))\n",
    "        y_lower += len(cluster_silhouette_vals)\n",
    "\n",
    "    # Get the average silhouette score and plot it\n",
    "    avg_score = np.mean(silhouette_vals)\n",
    "    ax1.axvline(avg_score, linestyle='--', linewidth=2, color='green')\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_xlabel('Silhouette coefficient values')\n",
    "    ax1.set_ylabel('Cluster labels')\n",
    "    ax1.set_title('Silhouette plot for the various clusters', y=1.02);\n",
    "    \n",
    "    # Use PCA to flatten the data\n",
    "    embedding = PCA(n_components=2)\n",
    "    pca = pd.DataFrame(embedding.fit_transform(X), columns = ['component1','component2'])\n",
    "    pca['labels'] = kmeans.predict(X)\n",
    "\n",
    "    # Scatter plot of data colored with labels\n",
    "    ax2.scatter(pca['component1'], pca['component2'], c=labels)\n",
    "    # ax2.scatter(centroids[:, 0], centroids[:, 1], marker='*', c='r', s=250)\n",
    "    # ax2.set_xlim([-2, 2])\n",
    "    # ax2.set_xlim([-2, 2])\n",
    "    ax2.set_xlabel('Component 1')\n",
    "    ax2.set_ylabel('Component 2')\n",
    "    ax2.set_title('Visualization of clustered data', y=1.02)\n",
    "    ax2.set_aspect('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Silhouette analysis using k = {k}',\n",
    "                 fontsize=16, fontweight='semibold', y=1.05);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data to plot with text and for clustering\n",
    "norm_df = pd.DataFrame(scale_norm_view_vec)\n",
    "norm_df['title'] = vec_df.title\n",
    "cluster_X = norm_df.iloc[:,:-2]\n",
    "kmeans = MiniBatchKMeans(n_clusters=9, init='k-means++', max_iter=300, random_state=0, batch_size=2048)\n",
    "labels = kmeans.fit_predict(cluster_X)\n",
    "norm_df['label'] = labels\n",
    "# sample 1000 data\n",
    "target_df = norm_df.sample(1000)\n",
    "X = target_df.iloc[:,:-3]\n",
    "labels = target_df.iloc[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca is showing promising cluster let's try t-SNE with text adjusting\n",
    "# Initialize t-SNE\n",
    "tsne = TSNE(n_components = 2, init = 'random', random_state = 0, perplexity = 75)\n",
    "\n",
    "# flatten the data and clustering then plot the cluster with t-SNE\n",
    "tsne_df = pd.DataFrame(tsne.fit_transform(X), columns = ['component1','component2'])\n",
    "# tsne_df['labels'] = labels\n",
    "# print(tsne_df)\n",
    "fig, ax = plt.subplots(figsize = (14, 10))\n",
    "# sns.scatterplot(tsne_df['component1'], tsne_df['component2'], alpha = 0.5, hue=labels)\n",
    "ax.scatter(tsne_df['component1'], tsne_df['component2'], c=labels)\n",
    "\n",
    "# use adjustText to help position the text\n",
    "from adjustText import adjust_text\n",
    "\n",
    "texts = []\n",
    "titles_to_plot = list(np.arange(0, 1000, 50)) # plots every 40th title in first 400 titles\n",
    "\n",
    "# Append words to list\n",
    "for title in titles_to_plot:\n",
    "    texts.append(plt.text(tsne_df.iloc[title, 0], tsne_df.iloc[title, 1], target_df.title.iloc[title], fontsize = 9))\n",
    "    \n",
    "# Plot text using adjust_text\n",
    "adjust_text(texts, force_points = 0.4, force_text = 0.5, \n",
    "            expand_points = (2,1), expand_text = (1,2),\n",
    "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = pd.DataFrame(scale_norm_view_vec)\n",
    "norm_df['title'] = vec_df.title\n",
    "cluster_X = norm_df.iloc[:,:-2]\n",
    "kmeans = MiniBatchKMeans(n_clusters=9, init='k-means++', max_iter=300, random_state=0, batch_size=2048)\n",
    "labels = kmeans.fit_predict(cluster_X)\n",
    "norm_df['label'] = labels\n",
    "# sample 800 data\n",
    "target_df = norm_df.sample(1000)\n",
    "X = target_df.iloc[:,:-3]\n",
    "labels = target_df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca is showing promising cluster let's try PCA with text adjusting\n",
    "# Initialize PCA\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "# flatten the data and clustering then plot the cluster with t-SNE\n",
    "pca_df = pd.DataFrame(pca.fit_transform(X), columns = ['component1','component2'])\n",
    "# pca_df['labels'] = labels\n",
    "# print(pca_df)\n",
    "fig, ax = plt.subplots(figsize = (14, 10))\n",
    "# sns.scatterplot(pca_df['component1'], pca_df['component2'], alpha = 0.5, hue=labels)\n",
    "ax.scatter(pca_df['component1'], pca_df['component2'], c=labels)\n",
    "\n",
    "# use adjustText to help position the text\n",
    "from adjustText import adjust_text\n",
    "\n",
    "texts = []\n",
    "titles_to_plot = list(np.arange(0, 1000, 50)) # plots every 40th title in first 400 titles\n",
    "\n",
    "# Append words to list\n",
    "for title in titles_to_plot:\n",
    "    texts.append(plt.text(pca_df.iloc[title, 0], pca_df.iloc[title, 1], target_df.title.iloc[title], fontsize = 9))\n",
    "    \n",
    "# Plot text using adjust_text\n",
    "adjust_text(texts, force_points = 0.4, force_text = 0.5, \n",
    "            expand_points = (2,1), expand_text = (1,2),\n",
    "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
    "plt.title('PCA clustering a 1,000 sampled data points with its text title')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try 3-D representation with t_SNE\n",
    "from wsgiref.headers import tspecials\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "# to create interactive 3D plot\n",
    "%matplotlib widget\n",
    "\n",
    "# Initialize t-SNE with 3 components for 3-dimensional plot\n",
    "tsne = TSNE(n_components = 3, init = 'random', random_state = 0, perplexity = 75)\n",
    "\n",
    "# flatten the data and clustering then plot the cluster with t-SNE\n",
    "tsne_df = pd.DataFrame(tsne.fit_transform(X), columns = ['component1','component2', 'component3'])\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(tsne_df['component1'], tsne_df['component2'], tsne_df['component3'], c=labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = pd.DataFrame(scale_norm_view_vec)\n",
    "norm_df['title'] = vec_df.title\n",
    "cluster_X = norm_df.iloc[:,:-2]\n",
    "kmeans = MiniBatchKMeans(n_clusters=9, init='k-means++', max_iter=300, random_state=0, batch_size=2048)\n",
    "labels = kmeans.fit_predict(cluster_X)\n",
    "norm_df['label'] = labels\n",
    "target_df = norm_df.sample(1000)\n",
    "X = target_df.iloc[:,:-3]\n",
    "labels = target_df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try 3-D representation with PCA\n",
    "\n",
    "# Initialize PCA with 3 components for 3-dimensional plot\n",
    "pca = PCA(n_components = 3)\n",
    "\n",
    "# flatten the data and clustering then plot the cluster with t-SNE\n",
    "pca_df = pd.DataFrame(pca.fit_transform(X), columns = ['component1','component2', 'component3'])\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(pca_df['component1'], pca_df['component2'], pca_df['component3'], c=labels)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "467f60c46b88ea196e8e4e51716c4f14b05141d893e1660e0038b81da854c476"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
