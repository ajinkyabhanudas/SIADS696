{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction\n",
    "extract essential information from JSON for the analysis\n",
    "\"title\",\"views\", \"likes\",\"comments\", 'duration','tag','description', 'channel', 'category', 'published'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting json to df\n",
    "import os, json, re\n",
    "import enum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from itertools import groupby\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize ,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# import nltk; nltk.download('popular')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "warnings.simplefilter(action='ignore')\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = 'data'\n",
    "feature_list = ['publishedAt', 'title', 'channelId', 'description',\n",
    "                'channelTitle', 'tags', 'categoryId', 'viewCount', \n",
    "                'likeCount', 'favoriteCount', 'commentCount', 'duration',\n",
    "                'definition', 'contentRating', 'topicCategories', 'topicLabel']\n",
    "\n",
    "ov_dict_list = []\n",
    "ov_temp_dict = {}\n",
    "\n",
    "dict_list = []\n",
    "temp_dict = {}\n",
    "\n",
    "for path in glob.glob(f'./{rootdir}/*/*'):\n",
    "  try:\n",
    "    with open(path, \"r\") as read_file:\n",
    "      data = json.load(read_file)\n",
    "    for channel_id in data.keys():\n",
    "      ov_temp_dict = data[channel_id][\"channel_statistics\"]\n",
    "      channel_name = list(data[channel_id][\"video_data\"].keys())[0]\n",
    "      ov_temp_dict[\"channelName\"] = data[channel_id][\"video_data\"][channel_name][\"channelTitle\"]\n",
    "      ov_dict_list.append(ov_temp_dict)\n",
    "      \n",
    "      for video_info in data[channel_id][\"video_data\"].values():\n",
    "        temp_dict = video_info\n",
    "        temp_dict[\"topicLabel\"] = path.split('\\\\')[1]\n",
    "        dict_list.append(temp_dict)       \n",
    "  except:\n",
    "    pass\n",
    "\n",
    "overall_stats_raw_df = pd.DataFrame(ov_dict_list)\n",
    "overall_stats_raw_df.drop(\"hiddenSubscriberCount\", axis=1, inplace=True)\n",
    "\n",
    "raw_df = pd.DataFrame(dict_list)\n",
    "raw_df = raw_df[feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55337, 16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_df.channelTitle.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration_split(duration):\n",
    "  try:\n",
    "    for _, v in groupby(duration, str.isalpha):\n",
    "      yield ''.join(v)\n",
    "  except:\n",
    "    yield np.nan\n",
    "    \n",
    "    \n",
    "def duration_2_secs(duration, duration_split=duration_split):\n",
    "  temp = 0\n",
    "  # value = (duration_split(duration))\n",
    "  for i, val in enumerate(duration_split(duration)):\n",
    "    \n",
    "    if len(str(duration)) <= 8:\n",
    "      if i==1 and not val.isalpha():\n",
    "        temp += float(val)*60\n",
    "      \n",
    "      if i==3 and not val.isalpha():\n",
    "        temp += float(val)\n",
    "    else:\n",
    "      if i==1 and not val.isalpha():\n",
    "        temp += float(val)*60*60\n",
    "      \n",
    "      if i==3 and not val.isalpha():\n",
    "        temp += float(val)*60\n",
    "      \n",
    "      if i==5 and not val.isalpha():\n",
    "        temp += float(val)\n",
    "\n",
    "  return(temp)\n",
    "\n",
    "\n",
    "# duration_2_secs(\"HR1PT51M12S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_extract(links_list):\n",
    "  \n",
    "  topics_list = []  \n",
    "  try:\n",
    "    for link in links_list:\n",
    "      topics_list.append(link.split(\"/\")[-1].lower())\n",
    "    \n",
    "    return(topics_list)\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prep(val):\n",
    "  '''the goal is to replace the hypertexts in the \n",
    "  in any field to redundant names as vectorizing\n",
    "  them could be misleading and also leak data'''\n",
    "  val = str(val).lower()\n",
    "  process_desc = re.sub(r'http[s]*:.*\\w', 'url', val)\n",
    "  process_desc = re.sub('[^a-zA-Z]', ' ', process_desc)\n",
    "  process_desc = re.sub(r'\\s+', ' ', process_desc)\n",
    "\n",
    "  process_desc = nltk.sent_tokenize(process_desc)\n",
    "  if not process_desc:\n",
    "    process_desc = [nltk.word_tokenize(word) for word in process_desc]\n",
    "  else:\n",
    "    process_desc = [nltk.word_tokenize(word) for word in process_desc][0]\n",
    "    \n",
    "  \n",
    "  word_list = [word for word in process_desc if word not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "  return word_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, featureset, primary=True):\n",
    "\n",
    "  if primary:\n",
    "    df[\"publishedAt\"] = pd.to_datetime(df.publishedAt, format='%Y-%m-%d')\n",
    "    df['publishedDayDelta'] = (datetime.now(timezone.utc) - df['publishedAt']).apply(lambda x: x.days)\n",
    "    df[\"categoryId\"] = df.categoryId.astype(float)\n",
    "    df[\"viewCount\"] = df.viewCount.astype(float)\n",
    "    df[\"likeCount\"] = df.likeCount.astype(float)\n",
    "    df[\"favoriteCount\"] = df.favoriteCount.astype(float)\n",
    "    df[\"commentCount\"] = df.commentCount.astype(float)\n",
    "    df['duration_secs'] = df.duration.apply(lambda x: duration_2_secs(x))\n",
    "    df['topicCategories'] = df.topicCategories.apply(lambda x: topic_extract(x))\n",
    "    df['channelTitle'] = df.channelTitle.str.lower()\n",
    "    df['topicLabel'] = df.topicLabel.str.lower()\n",
    "    df['log_duration_secs'] = np.log(df.duration_secs+1)\n",
    "  \n",
    "  else:\n",
    "    df['ov_viewCount'] = df.viewCount.astype(float)\n",
    "    df['ov_subscriberCount'] = df.subscriberCount.astype(float)\n",
    "    df['ov_videoCount'] = df.videoCount.astype(float)\n",
    "    df['channelName'] = df.channelName.str.lower()\n",
    "\n",
    "  return df[featureset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "  'title', 'description',\n",
    "  'channelTitle', 'categoryId', 'viewCount', \n",
    "  'log_duration_secs', 'topicCategories', 'publishedDayDelta', 'topicLabel'\n",
    " ]\n",
    "\n",
    "prep_df = create_dataset(raw_df, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                0.000000\n",
       "description          0.384914\n",
       "channelTitle         0.384914\n",
       "categoryId           0.384914\n",
       "viewCount            0.518640\n",
       "log_duration_secs    0.000000\n",
       "topicCategories      1.817952\n",
       "publishedDayDelta    0.000000\n",
       "topicLabel           0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pct_na = prep_df.isna().sum()/len(prep_df)*100\n",
    "pct_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.03025100746336"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pct_dropna = len(prep_df.dropna())/len(prep_df)*100\n",
    "pct_dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_df = prep_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(prep_df[['categoryId', 'viewCount', 'log_duration_secs', 'publishedDayDelta']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(prep_df.corr(), cmap='rocket_r', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Mining\n",
    "convert text into Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use gensim pretrained model called 'glove-wiki-gigaword-50' with the model trained on wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pretrained gensim model to convert text tokens into vector\n",
    "model_wv = api.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert token to word vector with word2vec finding average vector for the document\n",
    "# tokenize function with df series as input to give a list of list result of each title\n",
    "def df_token(df_text_series):\n",
    "    # set of english stopwords\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    token_item = []\n",
    "    for t in df_text_series:\n",
    "        if type(t) == list:\n",
    "            t = str(' '.join(t))\n",
    "        else:\n",
    "            pass\n",
    "        try:\n",
    "            tokens = [word for word in word_tokenize(t.lower()) if word not in stopset]\n",
    "        except:\n",
    "            tokens == None\n",
    "        token_item.append(tokens)\n",
    "    return token_item\n",
    "\n",
    "def gen_word_vec(df_text_series, wordvec):\n",
    "    word_vector = []\n",
    "    for token in df_token(df_text_series):\n",
    "        token_considered = [t for t in token if t.isalpha]\n",
    "        token_vocab = [i for i in token_considered if i in wordvec.key_to_index]\n",
    "        if len(token_vocab) > 0:\n",
    "            word_vector.append(np.mean(wordvec[token_vocab], axis=0))\n",
    "        else:\n",
    "            word_vector.append(np.zeros(wordvec.vector_size))\n",
    "    word_vector = np.array(word_vector)\n",
    "    return word_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_word_vec = gen_word_vec(prep_df.title, model_wv)\n",
    "desc_word_vec = gen_word_vec(prep_df.description, model_wv)\n",
    "topic_word_vec = gen_word_vec(prep_df.topicCategories, model_wv)\n",
    "label_word_vec = gen_word_vec(prep_df.topicLabel, model_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dimension of the average word vector for each title\n",
    "title_word_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracting cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "A = title_word_vec\n",
    "vectors = da.from_array(A, 10000)\n",
    "cosine = cosine_similarity(vectors)\n",
    "avg_cosine_dask = []\n",
    "for i in range(A.shape[0]):\n",
    "    avg_cosine_dask.append(np.mean(cosine[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce dimensionality using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate PCA with 1 components\n",
    "pca = PCA(n_components=1)\n",
    "title_pc = pca.fit_transform(title_word_vec)\n",
    "desc_pc = pca.fit_transform(desc_word_vec)\n",
    "topic_pc = pca.fit_transform(topic_word_vec)\n",
    "label_pc = pca.fit_transform(label_word_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy prep_df to be worked with\n",
    "vec_df = prep_df.copy()\n",
    "#replace df with each vector PCs\n",
    "vec_df['titleVec'] = title_pc\n",
    "vec_df['label'] = label_pc\n",
    "vec_df['description'] = desc_pc\n",
    "vec_df['topicCategories'] = topic_pc\n",
    "vec_df['avgTitleCosine'] =avg_cosine_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_vec = vec_df.drop(columns=['channelTitle', 'title', 'topicLabel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_vec.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_view_vec = vec_df.copy()\n",
    "norm_view_vec['averageView'] = norm_view_vec.viewCount/norm_view_vec.publishedDayDelta\n",
    "norm_view_vec['normalizedScore'] = norm_view_vec.groupby('topicLabel').averageView.apply(lambda x: x/x.median())\n",
    "norm_view_vec.drop(['channelTitle', 'viewCount', 'averageView' , 'title', 'topicLabel'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_view_vec.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the df to be fitted with PCA\n",
    "scaler = StandardScaler()\n",
    "scale_view_vec = scaler.fit_transform(view_vec)\n",
    "scale_norm_view_vec = scaler.fit_transform(norm_view_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_norm_view_df = pd.DataFrame(scale_norm_view_vec, columns=norm_view_vec.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(scale_norm_view_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "sns.heatmap(ax=ax, data=scale_norm_view_df.corr(), cmap='coolwarm_r', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create biplot source from https://sukhbinder.wordpress.com/2015/08/05/biplot-with-python/ + unsupervise learning class\n",
    "def biplot(score, coeff, maxdim, pcax=1, pcay=2, labels=None):\n",
    "    '''\n",
    "    score: pca fit_transform\n",
    "    coeff: components.transpose\n",
    "    '''\n",
    "    pca1=pcax-1\n",
    "    pca2=pcay-1\n",
    "    xs = score[:,pca1]\n",
    "    ys = score[:,pca2]\n",
    "    n = min(coeff.shape[0], maxdim)\n",
    "    scalex = 2.0/(xs.max()- xs.min())\n",
    "    scaley = 2.0/(ys.max()- ys.min())\n",
    "    text_scale_factor = 1.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(xs*scalex, ys*scaley, s=1)\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coeff[i,pca1], coeff[i,pca2],color='r',alpha=0.5) \n",
    "        if labels is None:\n",
    "            plt.text(coeff[i,pca1]* text_scale_factor, coeff[i,pca2] * text_scale_factor, \"Var\"+str(i+1), color='k', ha='center', va='center')\n",
    "        else:\n",
    "            plt.text(coeff[i,pca1]* text_scale_factor, coeff[i,pca2], labels[i], color='m', ha='center', va='center')\n",
    "    plt.xlim(-1,1)\n",
    "    plt.ylim(-1,1)\n",
    "    plt.xlabel(\"PC{}\".format(pcax))\n",
    "    plt.ylabel(\"PC{}\".format(pcay))\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biplot with viewCount variable\n",
    "score_pca = PCA(n_components=2)\n",
    "score = score_pca.fit_transform(scale_view_vec)\n",
    "labels = view_vec.columns\n",
    "coeff = np.transpose(score_pca.components_)\n",
    "biplot(score,coeff, maxdim=12, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#biplot normalizedScore variable\n",
    "score_pca = PCA(n_components=8).fit(scale_norm_view_vec)\n",
    "score = score_pca.transform(scale_norm_view_vec)\n",
    "labels = scale_norm_view_df.columns\n",
    "coeff = np.transpose(score_pca.components_)\n",
    "biplot(score,coeff, maxdim=12, labels=labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scree plot\n",
    "PC_values = np.arange(score_pca.n_components_) + 1\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(PC_values, score_pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the clustering of word embadding vector using K-Means\n",
    "# first let's find optimal number of cluster using elbow methods\n",
    "\n",
    "# workingin title word embedding\n",
    "sse=[] # sum of square error\n",
    "list_k = list(range(1, 50))\n",
    "\n",
    "X = scale_norm_view_vec\n",
    "for k in list_k:\n",
    "    km = MiniBatchKMeans(n_clusters=k, init='k-means++', max_iter=300, random_state=0, batch_size=2048)\n",
    "    km.fit(X)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore cluster with yellowbrick KElbowVisualizer\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "X = scale_norm_view_vec\n",
    "model = MiniBatchKMeans(n_clusters=k, init='k-means++', max_iter=300, random_state=0, batch_size=2048)\n",
    "visualizer = KElbowVisualizer(model, k=(3,13))\n",
    "visualizer.fit(X)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result from the elbow method shows that the optimum number of KNN is 9, so let's explore further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silhouette score plot with yellowbrick\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "model_k = MiniBatchKMeans(n_clusters=9, init='k-means++', max_iter=300, random_state=0, batch_size=2048)\n",
    "visualizer_silhouette = SilhouetteVisualizer(model_k, colors='yellowbrick')\n",
    "visualizer_silhouette.fit(X)\n",
    "visualizer_silhouette.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use silhouette plot based on sklearn documentation page https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "#set the dataset to be working with\n",
    "X = scale_norm_view_vec\n",
    "\n",
    "for i, k in enumerate([5, 6, 7, 8, 9, 10, 11]):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "    \n",
    "    # Run the Kmeans algorithm\n",
    "    kmeans = MiniBatchKMeans(n_clusters=k, init='k-means++', max_iter=300, random_state=0, batch_size=2048)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    # centroids = kmeans.cluster_centers_\n",
    "\n",
    "    # Get silhouette samples\n",
    "    silhouette_vals = silhouette_samples(X, labels)\n",
    "\n",
    "    # Silhouette plot\n",
    "    y_ticks = []\n",
    "    y_lower, y_upper = 0, 0\n",
    "    for i, cluster in enumerate(np.unique(labels)):\n",
    "        cluster_silhouette_vals = silhouette_vals[labels == cluster]\n",
    "        cluster_silhouette_vals.sort()\n",
    "        y_upper += len(cluster_silhouette_vals)\n",
    "        ax1.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none', height=1)\n",
    "        ax1.text(-0.03, (y_lower + y_upper) / 2, str(i + 1))\n",
    "        y_lower += len(cluster_silhouette_vals)\n",
    "\n",
    "    # Get the average silhouette score and plot it\n",
    "    avg_score = np.mean(silhouette_vals)\n",
    "    ax1.axvline(avg_score, linestyle='--', linewidth=2, color='green')\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_xlabel('Silhouette coefficient values')\n",
    "    ax1.set_ylabel('Cluster labels')\n",
    "    ax1.set_title('Silhouette plot for the various clusters', y=1.02);\n",
    "    \n",
    "    # Use PCA to flatten the data\n",
    "    embedding = PCA(n_components=2)\n",
    "    pca = pd.DataFrame(embedding.fit_transform(X), columns = ['component1','component2'])\n",
    "    pca['labels'] = kmeans.predict(X)\n",
    "\n",
    "    # Scatter plot of data colored with labels\n",
    "    ax2.scatter(pca['component1'], pca['component2'], c=labels)\n",
    "    # ax2.scatter(centroids[:, 0], centroids[:, 1], marker='*', c='r', s=250)\n",
    "    # ax2.set_xlim([-2, 2])\n",
    "    # ax2.set_xlim([-2, 2])\n",
    "    ax2.set_xlabel('Component 1')\n",
    "    ax2.set_ylabel('Component 2')\n",
    "    ax2.set_title('Visualization of clustered data', y=1.02)\n",
    "    ax2.set_aspect('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Silhouette analysis using k = {k}',\n",
    "                 fontsize=16, fontweight='semibold', y=1.05);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data to plot with text and for clustering\n",
    "norm_df = pd.DataFrame(scale_norm_view_vec)\n",
    "norm_df['title'] = vec_df.title\n",
    "cluster_X = norm_df.iloc[:,:-2]\n",
    "kmeans = MiniBatchKMeans(n_clusters=9, init='k-means++', max_iter=300, random_state=0, batch_size=2048)\n",
    "labels = kmeans.fit_predict(cluster_X)\n",
    "norm_df['label'] = labels\n",
    "# sample 1000 data\n",
    "target_df = norm_df.sample(1000)\n",
    "X = target_df.iloc[:,:-3]\n",
    "labels = target_df.iloc[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca is showing promising cluster let's try t-SNE with text adjusting\n",
    "# Initialize t-SNE\n",
    "tsne = TSNE(n_components = 2, init = 'random', random_state = 0, perplexity = 75)\n",
    "\n",
    "# flatten the data and clustering then plot the cluster with t-SNE\n",
    "tsne_df = pd.DataFrame(tsne.fit_transform(X), columns = ['component1','component2'])\n",
    "# tsne_df['labels'] = labels\n",
    "# print(tsne_df)\n",
    "fig, ax = plt.subplots(figsize = (14, 10))\n",
    "# sns.scatterplot(tsne_df['component1'], tsne_df['component2'], alpha = 0.5, hue=labels)\n",
    "ax.scatter(tsne_df['component1'], tsne_df['component2'], c=labels)\n",
    "\n",
    "# use adjustText to help position the text\n",
    "from adjustText import adjust_text\n",
    "\n",
    "texts = []\n",
    "titles_to_plot = list(np.arange(0, 1000, 50)) # plots every 40th title in first 400 titles\n",
    "\n",
    "# Append words to list\n",
    "for title in titles_to_plot:\n",
    "    texts.append(plt.text(tsne_df.iloc[title, 0], tsne_df.iloc[title, 1], target_df.title.iloc[title], fontsize = 9))\n",
    "    \n",
    "# Plot text using adjust_text\n",
    "adjust_text(texts, force_points = 0.4, force_text = 0.5, \n",
    "            expand_points = (2,1), expand_text = (1,2),\n",
    "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = pd.DataFrame(scale_norm_view_vec)\n",
    "norm_df['title'] = vec_df.title\n",
    "cluster_X = norm_df.iloc[:,:-2]\n",
    "kmeans = MiniBatchKMeans(n_clusters=9, init='k-means++', max_iter=300, random_state=0, batch_size=2048)\n",
    "labels = kmeans.fit_predict(cluster_X)\n",
    "norm_df['label'] = labels\n",
    "# sample 800 data\n",
    "target_df = norm_df.sample(1000)\n",
    "X = target_df.iloc[:,:-3]\n",
    "labels = target_df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca is showing promising cluster let's try PCA with text adjusting\n",
    "# Initialize PCA\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "# flatten the data and clustering then plot the cluster with t-SNE\n",
    "pca_df = pd.DataFrame(pca.fit_transform(X), columns = ['component1','component2'])\n",
    "# pca_df['labels'] = labels\n",
    "# print(pca_df)\n",
    "fig, ax = plt.subplots(figsize = (14, 10))\n",
    "# sns.scatterplot(pca_df['component1'], pca_df['component2'], alpha = 0.5, hue=labels)\n",
    "ax.scatter(pca_df['component1'], pca_df['component2'], c=labels)\n",
    "\n",
    "# use adjustText to help position the text\n",
    "from adjustText import adjust_text\n",
    "\n",
    "texts = []\n",
    "titles_to_plot = list(np.arange(0, 1000, 50)) # plots every 40th title in first 400 titles\n",
    "\n",
    "# Append words to list\n",
    "for title in titles_to_plot:\n",
    "    texts.append(plt.text(pca_df.iloc[title, 0], pca_df.iloc[title, 1], target_df.title.iloc[title], fontsize = 9))\n",
    "    \n",
    "# Plot text using adjust_text\n",
    "adjust_text(texts, force_points = 0.4, force_text = 0.5, \n",
    "            expand_points = (2,1), expand_text = (1,2),\n",
    "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
    "plt.title('PCA clustering a 1,000 sampled data points with its text title')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try 3-D representation with t_SNE\n",
    "from wsgiref.headers import tspecials\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "# to create interactive 3D plot\n",
    "%matplotlib widget\n",
    "\n",
    "# Initialize t-SNE with 3 components for 3-dimensional plot\n",
    "tsne = TSNE(n_components = 3, init = 'random', random_state = 0, perplexity = 75)\n",
    "\n",
    "# flatten the data and clustering then plot the cluster with t-SNE\n",
    "tsne_df = pd.DataFrame(tsne.fit_transform(X), columns = ['component1','component2', 'component3'])\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(tsne_df['component1'], tsne_df['component2'], tsne_df['component3'], c=labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = pd.DataFrame(scale_norm_view_vec)\n",
    "norm_df['title'] = vec_df.title\n",
    "cluster_X = norm_df.iloc[:,:-2]\n",
    "kmeans = MiniBatchKMeans(n_clusters=9, init='k-means++', max_iter=300, random_state=0, batch_size=2048)\n",
    "labels = kmeans.fit_predict(cluster_X)\n",
    "norm_df['label'] = labels\n",
    "target_df = norm_df.sample(1000)\n",
    "X = target_df.iloc[:,:-3]\n",
    "labels = target_df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try 3-D representation with PCA\n",
    "\n",
    "# Initialize PCA with 3 components for 3-dimensional plot\n",
    "pca = PCA(n_components = 3)\n",
    "\n",
    "# flatten the data and clustering then plot the cluster with t-SNE\n",
    "pca_df = pd.DataFrame(pca.fit_transform(X), columns = ['component1','component2', 'component3'])\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(pca_df['component1'], pca_df['component2'], pca_df['component3'], c=labels)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "467f60c46b88ea196e8e4e51716c4f14b05141d893e1660e0038b81da854c476"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
